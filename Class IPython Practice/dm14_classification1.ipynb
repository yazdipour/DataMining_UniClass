{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Classification (Spam filtering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: http://zacstewart.com/2015/04/28/document-classification-with-scikit-learn.html\n",
    "\n",
    "Spam filtering is kind of like the \"Hello world\" of document classification. It's a binary classification problem: either spam, or not spam (a.k.a ham)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download data from http://www2.aueb.gr/users/ion/data/enron-spam/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll work through to following tasks to get there:\n",
    "\n",
    "- Loading raw email data into a workable format\n",
    "- Extracting features from the raw data that an algorithm can learn from\n",
    "- Training a classifier\n",
    "- Evaluating accuracy by cross-validation\n",
    "- Improving upon initial accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to protocol, email headers and bodies are separated by a blank line (NEWLINE), so we simply ignore all lines before that and then yield the rest of the email. You'll also notice the encoding=\"latin-1\" bit. Some of the corpus is not in Unicode, so this makes a \"best effort\" attempt to decode the files correctly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(About function os.walk used in the following code:\n",
    "\n",
    "os.walk(top, topdown=True, onerror=None, followlinks=False)\n",
    "Generate the file names in a directory tree by walking the tree either top-down or bottom-up. For each directory in the tree rooted at directory top (including top itself), it yields a 3-tuple (dirpath, dirnames, filenames).\n",
    "\n",
    "dirpath is a string, the path to the directory. dirnames is a list of the names of the subdirectories in dirpath (excluding '.' and '..'). filenames is a list of the names of the non-directory files in dirpath. Note that the names in the lists contain no path components. To get a full path (which begins with top) to a file or directory in dirpath, do os.path.join(dirpath, name). )\n",
    "\n",
    "(About method str.join used in the following code:\n",
    "It takes a list of strings and joins them, seprated by the first string.\n",
    "For example \";\".join([\"A\",\"B\",\"C\"]) is \"A;B;C\" .\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "SKIP_FILES = {'cmds'}\n",
    "\n",
    "def read_files(path):\n",
    "    for root, dir_names, file_names in os.walk(path):\n",
    "        for path in dir_names:\n",
    "            read_files(os.path.join(root, path))\n",
    "        for file_name in file_names:\n",
    "            if file_name not in SKIP_FILES:\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                past_header, lines = False, []\n",
    "                f = open(file_path, encoding=\"latin-1\")\n",
    "                for line in f:\n",
    "                    if past_header:\n",
    "                        lines.append(line)\n",
    "                    elif line == '\\n':\n",
    "                        past_header = True\n",
    "                f.close()\n",
    "                content = '\\n'.join(lines)\n",
    "                yield file_path, content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to build a dataset from all these email bodies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build data frame of texts and labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rows = []\n",
    "index = []\n",
    "for file_path, text in read_files(\".\\\\emails\"):\n",
    "    if file_path.find(\"emails\\\\spam\") != -1:\n",
    "        classification = 1\n",
    "    else:\n",
    "        classification = 0\n",
    "    rows.append({'text': text, 'class': classification})\n",
    "    index.append(file_path)\n",
    "\n",
    "data_frame = DataFrame(rows, index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>.\\emails\\ham\\beck-s\\2001_plan\\1</th>\n",
       "      <td>0</td>\n",
       "      <td>Guys, attached you will find a final cut on th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.\\emails\\ham\\beck-s\\2001_plan\\2</th>\n",
       "      <td>0</td>\n",
       "      <td>I am still in need of the information regardin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.\\emails\\ham\\beck-s\\2001_plan\\3</th>\n",
       "      <td>0</td>\n",
       "      <td>Attached is a file containing all cost centers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.\\emails\\ham\\beck-s\\2001_plan\\4</th>\n",
       "      <td>0</td>\n",
       "      <td>Sarah,\\n\\nBelow is our high level explanation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.\\emails\\ham\\beck-s\\2001_plan\\5</th>\n",
       "      <td>0</td>\n",
       "      <td>Sally - I will check into the meaning of this ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 class  \\\n",
       ".\\emails\\ham\\beck-s\\2001_plan\\1      0   \n",
       ".\\emails\\ham\\beck-s\\2001_plan\\2      0   \n",
       ".\\emails\\ham\\beck-s\\2001_plan\\3      0   \n",
       ".\\emails\\ham\\beck-s\\2001_plan\\4      0   \n",
       ".\\emails\\ham\\beck-s\\2001_plan\\5      0   \n",
       "\n",
       "                                                                              text  \n",
       ".\\emails\\ham\\beck-s\\2001_plan\\1  Guys, attached you will find a final cut on th...  \n",
       ".\\emails\\ham\\beck-s\\2001_plan\\2  I am still in need of the information regardin...  \n",
       ".\\emails\\ham\\beck-s\\2001_plan\\3  Attached is a file containing all cost centers...  \n",
       ".\\emails\\ham\\beck-s\\2001_plan\\4  Sarah,\\n\\nBelow is our high level explanation ...  \n",
       ".\\emails\\ham\\beck-s\\2001_plan\\5  Sally - I will check into the meaning of this ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_frame = data_frame.reindex(np.random.permutation(data_frame.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>.\\emails\\spam\\GP\\part10\\msg1271.eml</th>\n",
       "      <td>1</td>\n",
       "      <td>&lt;!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 3.2//E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.\\emails\\spam\\GP\\part11\\msg3650.eml</th>\n",
       "      <td>1</td>\n",
       "      <td>This is a multi-part message in MIME format.\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.\\emails\\spam\\GP\\part4\\msg7242.eml</th>\n",
       "      <td>1</td>\n",
       "      <td>&lt;!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 3.2//E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.\\emails\\spam\\GP\\part12\\msg3805.eml</th>\n",
       "      <td>1</td>\n",
       "      <td>&lt;!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 3.2//E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.\\emails\\spam\\GP\\part6\\msg10215.eml</th>\n",
       "      <td>1</td>\n",
       "      <td>&lt;!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.0 Tr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     class  \\\n",
       ".\\emails\\spam\\GP\\part10\\msg1271.eml      1   \n",
       ".\\emails\\spam\\GP\\part11\\msg3650.eml      1   \n",
       ".\\emails\\spam\\GP\\part4\\msg7242.eml       1   \n",
       ".\\emails\\spam\\GP\\part12\\msg3805.eml      1   \n",
       ".\\emails\\spam\\GP\\part6\\msg10215.eml      1   \n",
       "\n",
       "                                                                                  text  \n",
       ".\\emails\\spam\\GP\\part10\\msg1271.eml  <!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 3.2//E...  \n",
       ".\\emails\\spam\\GP\\part11\\msg3650.eml  This is a multi-part message in MIME format.\\n...  \n",
       ".\\emails\\spam\\GP\\part4\\msg7242.eml   <!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 3.2//E...  \n",
       ".\\emails\\spam\\GP\\part12\\msg3805.eml  <!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 3.2//E...  \n",
       ".\\emails\\spam\\GP\\part6\\msg10215.eml  <!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.0 Tr...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer() #New object of class CountVectorizer\n",
    "counts = count_vectorizer.fit_transform(data_frame['text'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll instantiate a CountVectorizer and then call its instance method fit_transform, which does two things: it learns the vocabulary in all emails and extracts word count features. This method is an efficient way to do both steps, and for us it does the job. However, in some cases you may want to use a different vocabulary than the one inherent in the raw data. For this reason, CountVectorizer provides fit and transform methods to do them separately. Additionally, you can provide a vocabulary in the constructor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       A\n",
      "0  hello world bye hello\n",
      "1                bye you\n",
      "  (0, 1)\t2\n",
      "  (0, 2)\t1\n",
      "  (0, 0)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 3)\t1\n"
     ]
    }
   ],
   "source": [
    "dd = DataFrame([{\"A\": \"hello world bye hello\"}, {\"A\": \"bye you\" }])\n",
    "print(dd)\n",
    "result = CountVectorizer().fit_transform(dd['A'].values)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Emails (Naive Bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "targets = data_frame['class'].values\n",
    "classifier.fit(counts, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there we have it: a trained spam classifier. We can try it out by constructing some examples and predicting on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 177631)\t1\n",
      "  (0, 266268)\t1\n",
      "  (0, 540915)\t1\n",
      "  (0, 570859)\t1\n",
      "  (1, 145540)\t1\n",
      "  (1, 282034)\t1\n",
      "  (1, 284997)\t1\n",
      "  (1, 372175)\t1\n",
      "  (1, 535695)\t1\n",
      "  (1, 540722)\t1\n",
      "  (1, 541321)\t1\n",
      "  (1, 560873)\t1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = ['Free Viagra call today!', \"I'm going to attend the Linux users group tomorrow.\"]\n",
    "example_counts = count_vectorizer.transform(examples)\n",
    "print(example_counts)\n",
    "predictions = classifier.predict(example_counts)\n",
    "predictions # [1, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still, doing each one of those steps one-at-a-time was pretty tedious. We can package it all up using a construct provided by scikit-learn called a Pipeline.\n",
    "\n",
    "A pipeline does exactly what it sounds like: connects a series of steps into one object which you train and then use to make predictions. In short, we can use a pipeline to merge the feature extraction and classification into one operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer',  CountVectorizer()),\n",
    "    ('classifier',  MultinomialNB()) ])\n",
    "\n",
    "pipeline.fit(data_frame['text'].values, data_frame['class'].values)\n",
    "pipeline.predict(examples) # ['spam', 'ham']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "k_fold = KFold(n=len(data_frame), n_folds=6)\n",
    "scores = []\n",
    "confusion = np.array([[0, 0], [0, 0]])\n",
    "for train_indices, test_indices in k_fold:\n",
    "    train_text = data_frame.iloc[train_indices]['text'].values\n",
    "    train_y = data_frame.iloc[train_indices]['class'].values\n",
    "\n",
    "    test_text = data_frame.iloc[test_indices]['text'].values\n",
    "    test_y = data_frame.iloc[test_indices]['class'].values\n",
    "\n",
    "    pipeline.fit(train_text, train_y)\n",
    "    predictions = pipeline.predict(test_text)\n",
    "\n",
    "    confusion += confusion_matrix(test_y, predictions)\n",
    "    score = f1_score(test_y, predictions, pos_label=1)\n",
    "    scores.append(score)\n",
    "\n",
    "print('Total emails classified:', len(data_frame))\n",
    "print('Score:', sum(scores)/len(scores))\n",
    "print('Confusion matrix:')\n",
    "print(confusion)\n",
    "\n",
    "# Total emails classified: 55326\n",
    "# Score: 0.942661080942\n",
    "# Confusion matrix:\n",
    "# [[21660   178]\n",
    "#  [ 3473 30015]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn provides various functions for evaluating the accuracy of a model. We're using the F1 score for each fold, which we then average together for a mean accuracy on the entire set. Using the model we just built and the example data sets mentioned in the beginning of this tutorial, we get about 0.94. A confusion matrix helps elucidate how the model did for individual classes. Out of 55,326 examples, we get about 178 false spams, and 3,473 false hams. I say \"about\" because by shuffling the data as we did, these numbers will vary each time we run the model.\n",
    "\n",
    "That's really not bad for a first run. Obviously it's not production-ready even if we don't consider the scaling issues. Consumers demand accuracy, especially regarding false spams. Who doesn't hate to lose something important to the spam filter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get better results, there's a few things we can change. We can try to extract more features from the emails, we can try different kinds of features, we can tune the parameters of the naÃ¯ve Bayes classifier, or try another classifier all together.\n",
    "\n",
    "One way to get more features is to use n-gram counts instead of just word counts. So far we've relied upon what's known as \"bag of words\" features. It's called that because we simply toss all the words of a document into a \"bag\" and count them, disregarding any meaning that could locked up in the ordering of words.\n",
    "\n",
    "An n-gram can be thought of as a phrase that is n words long. For example, in the sentence \"Don't tase me, bro\" we have the 1-grams, \"don't,\" \"tase,\" \"me,\" and \"bro.\" The same sentence also has the 2-grams (or bigrams) \"don't tase\", \"tase me\", and \"me bro.\" We can tell CountVectorizer to include any order of n-grams by giving it a range. For this data set, unigrams and bigrams seem to work well. 3-grams add a tiny bit more accuracy, but for the computation time they incur, it's probably not worth the marginal increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('count_vectorizer', CountVectorizer(ngram_range=(1, 2))),\n",
    "    ('classifier',       MultinomialNB())\n",
    "])\n",
    "\n",
    "scores = []\n",
    "confusion = np.array([[0, 0], [0, 0]])\n",
    "for train_indices, test_indices in k_fold:\n",
    "    train_text = data_frame.iloc[train_indices]['text'].values\n",
    "    train_y = data_frame.iloc[train_indices]['class'].values\n",
    "\n",
    "    test_text = data_frame.iloc[test_indices]['text'].values\n",
    "    test_y = data_frame.iloc[test_indices]['class'].values\n",
    "\n",
    "    pipeline.fit(train_text, train_y)\n",
    "    predictions = pipeline.predict(test_text)\n",
    "\n",
    "    confusion += confusion_matrix(test_y, predictions)\n",
    "    score = f1_score(test_y, predictions, pos_label=1)\n",
    "    scores.append(score)\n",
    "\n",
    "print('Total emails classified:', len(data_frame))\n",
    "print('Score:', sum(scores)/len(scores))\n",
    "print('Confusion matrix:')\n",
    "print(confusion)\n",
    "\n",
    "# Total emails classified: 55326\n",
    "# Score: 0.978154601119\n",
    "# Confusion matrix:\n",
    "# [[21745    93]\n",
    "#  [ 1343 32145]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That boosts the model up to an F1 score of about 0.98. As before, it's a good idea to keep an eye on how it's doing for individual classes and not just the set as a whole. Luckily this increase represents an increase for both spam and ham classification accuracy.\n",
    "\n",
    "Another way to improve accuracy is to use different kinds of features. N-gram counts have the disadvantage of unfairly weighting longer documents. A six-word spammy message and a five-page, heartfelt letter with six \"spammy\" words could potentially receive the same \"spamminess\" probability. To counter that, we can use frequencies rather than occurances. That is, focusing on how much of the document is made up of a particular word, instead of how many times the word appears in the document. This kind of feature set is known as term frequencies.\n",
    "\n",
    "In addition to converting counts to frequencies, we can reduce noise in the features by reducing the weight for words that are common across the entire corpus. For example, words like \"and,\" \"the,\" and \"but\" probably don't contain a lot of information about the topic of the document, even though they will have high counts and frequencies across both ham and spam. To remedy that, we can use what's known as inverse document frequency or IDF.\n",
    "\n",
    "--> See the reference for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
